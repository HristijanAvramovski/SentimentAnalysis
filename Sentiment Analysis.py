# -*- coding: utf-8 -*-
"""Untitled15.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FCGHVeATA-UfJou_VJMa3KIWWVMasmQ4
"""



from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer
from sklearn.model_selection import train_test_split
from keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import keras.backend as K
from keras.models import Sequential
from keras.layers import Dense,Embedding,Conv1D,MaxPooling1D,LSTM, Flatten
from sklearn.metrics import accuracy_score,confusion_matrix,classification_report, r2_score
import tensorflow as tf
import seaborn as sns
import numpy as np
from sklearn.metrics import classification_report
import matplotlib.pyplot as plt

pip install --upgrade matplotlib

plt.style.use('default')

nltk.download('vader_lexicon')

longcovid_tweets=pd.read_csv('/content/drive/MyDrive/lc2020.csv',encoding='unicode_escape')

longcovid_tweets.head()

def preprocess_tweet(text):
    # Convert to lowercase
    text = text.lower()
    # Remove URLs
    text = ' '.join(word for word in text.split() if not word.startswith('http'))
    # Remove user mentions and hashtags
    text = ' '.join(word for word in text.split() if not word.startswith('@'))
    text = ' '.join(word for word in text.split() if not word.startswith('#'))
    # Remove special characters and punctuation
    text = ''.join(c for c in text if c.isalnum() or c.isspace())
    return text

sid=SentimentIntensityAnalyzer()

sentiment_labels=[]
for tweet in longcovid_tweets['Tweets'].tolist():
  preprocessed_tweet=preprocess_tweet(tweet)
  sentiment_scores=sid.polarity_scores(preprocessed_tweet)
  sentiment = 'positive' if sentiment_scores['compound'] > 0 else 'negative' if sentiment_scores['compound'] < 0 else 'neutral'
  sentiment_labels.append(sentiment)

longcovid_tweets['Sentiment']=sentiment_labels

longcovid_tweets.head()

longcovid_tweets.isnull().sum()

longcovid_tweets['text_length'] = longcovid_tweets['Tweets'].apply(len)

longcovid_tweets.head()

longcovid_tweets['text_length'].hist(bins=50)

g = sns.FacetGrid(longcovid_tweets,col='Sentiment')
g.map(plt.hist,'text_length')

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
sns.countplot(longcovid_tweets['Sentiment'])

X = longcovid_tweets['Tweets']
y = pd.get_dummies(longcovid_tweets['Sentiment']).values
num_classes = longcovid_tweets['Sentiment'].nunique()

X_train, X_test, Y_train, Y_test = train_test_split(X,y,test_size=0.2,random_state=1)
print(X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)

X_train

max_features = 20000
tokenizer = Tokenizer(num_words=max_features)
tokenizer.fit_on_texts(list(X_train))
X_train = tokenizer.texts_to_sequences(X_train)
X_test = tokenizer.texts_to_sequences(X_test)

totalNumWords = [len(one_comment) for one_comment in X_train]
plt.hist(totalNumWords,bins = 30)
plt.show()

max_words = 42
X_train = pad_sequences(X_train, maxlen=max_words)
X_test = pad_sequences(X_test, maxlen=max_words)
print(X_train.shape,X_test.shape)

K.clear_session()
model = Sequential()
model.add(Embedding(max_features, 42, input_length=X_train.shape[1]))
model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))
model.add(MaxPooling1D(pool_size=2))
model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))
model.add(MaxPooling1D(pool_size=2))
model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))
model.add(Dense(num_classes, activation='softmax'))

model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

model_history = model.fit(X_train, Y_train,  epochs=10, batch_size=128, verbose=2, validation_split=0.2)

model.save('/content/drive/MyDrive/model_cnn_lstm')

model = tf.keras.models.load_model('/content/drive/MyDrive/model_cnn_lstm')

y_pred = model.predict(X_test)

y_pred = y_pred.round()

y_pred.astype(int)



plt.plot(model_history.history['loss'])
plt.plot(model_history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper left')
plt.show()

plt.figure(figsize=(10, 6))  # Adjust the figure size as needed
plt.plot(y_pred, label='Predicted', color='blue')  # Set line color
plt.plot(Y_test, label='True', color='green')  # Set line color
plt.xlabel('Sample Index')
plt.ylabel('Values')
plt.title('Predicted vs True Values')
plt.legend()
plt.grid(True)  # Add grid lines

# Set background color to white
ax = plt.gca()
ax.set_facecolor('white')

plt.show()

print(classification_report(np.argmax(Y_test,axis=1), np.argmax(y_pred,axis=1)))







